{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:28.908035Z",
     "iopub.status.busy": "2022-07-06T23:16:28.907564Z",
     "iopub.status.idle": "2022-07-06T23:16:32.570258Z",
     "shell.execute_reply": "2022-07-06T23:16:32.567740Z",
     "shell.execute_reply.started": "2022-07-06T23:16:28.907940Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as AF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torchaudio.set_audio_backend(\"sox_io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:32.591524Z",
     "iopub.status.busy": "2022-07-06T23:16:32.589000Z",
     "iopub.status.idle": "2022-07-06T23:16:32.642384Z",
     "shell.execute_reply": "2022-07-06T23:16:32.640456Z",
     "shell.execute_reply.started": "2022-07-06T23:16:32.591483Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum((y_true * y_pred) > 0.5) / torch.sum(y_true)\n",
    "\n",
    "def stem(file):\n",
    "    return file.split(\"/\")[-1]\n",
    "\n",
    "def find_files(directory, pattern='**/*.wav'):\n",
    "    \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "    return glob(os.path.join(directory, pattern), recursive=True)\n",
    "\n",
    "def seed_everywhere():\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self, name:str, momentum=0.95):\n",
    "        assert 0.0 <= momentum < 1.0\n",
    "        \n",
    "        self.name = name\n",
    "        self.momentum = momentum\n",
    "        self.x = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(\"{} -> {:0.3f}\".format(self.name, self.x))\n",
    "    \n",
    "    def get(self):\n",
    "        return self.x\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x = None\n",
    "        \n",
    "    def step(self, x: float):\n",
    "        if self.x is None:\n",
    "            self.x = x\n",
    "        else:\n",
    "            self.x = self.momentum * self.x + (1 - self.momentum) * x\n",
    "            \n",
    "class Timer(object):\n",
    "    def __init__(self):\n",
    "        self._tic = time.time()\n",
    "\n",
    "    def tic(self):\n",
    "        self._tic = time.time()\n",
    "        return self._tic\n",
    "\n",
    "    def toc(self):\n",
    "        return time.time() - self._tic\n",
    "\n",
    "    def tictoc(self):\n",
    "        _toc = time.time()\n",
    "        duration = _toc - self._tic\n",
    "        self._tic = _toc\n",
    "        return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:32.645134Z",
     "iopub.status.busy": "2022-07-06T23:16:32.644259Z",
     "iopub.status.idle": "2022-07-06T23:16:32.652371Z",
     "shell.execute_reply": "2022-07-06T23:16:32.651042Z",
     "shell.execute_reply.started": "2022-07-06T23:16:32.645094Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sound:\n",
    "    def __init__(self, \n",
    "                 path, \n",
    "                 sound_id=None,\n",
    "                 meta=None, \n",
    "                 samplerate=None):\n",
    "        self.path = str(path)\n",
    "        self.sound_id = sound_id\n",
    "        self.samplerate = samplerate\n",
    "        self.meta = meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Model and Loss\n",
    "link: https://arxiv.org/abs/2005.07143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:38.325167Z",
     "iopub.status.busy": "2022-07-06T23:16:38.324790Z",
     "iopub.status.idle": "2022-07-06T23:16:38.365339Z",
     "shell.execute_reply": "2022-07-06T23:16:38.364321Z",
     "shell.execute_reply.started": "2022-07-06T23:16:38.325132Z"
    }
   },
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, bottleneck=128):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(bottleneck),\n",
    "            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.se(input)\n",
    "        return input * x\n",
    "\n",
    "class Bottle2neck(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=None, dilation=None, scale = 4):\n",
    "\n",
    "        super(Bottle2neck, self).__init__()\n",
    "\n",
    "        width       = int(math.floor(planes / scale))\n",
    "        \n",
    "        self.conv1  = nn.Conv1d(inplanes, width*scale, kernel_size=1)\n",
    "        self.bn1    = nn.BatchNorm1d(width*scale)\n",
    "        \n",
    "        self.nums   = scale -1\n",
    "\n",
    "        convs       = []\n",
    "        bns         = []\n",
    "\n",
    "        num_pad = math.floor(kernel_size/2)*dilation\n",
    "\n",
    "        for i in range(self.nums):\n",
    "            convs.append(nn.Conv1d(width, width, kernel_size=kernel_size, dilation=dilation, padding=num_pad))\n",
    "            bns.append(nn.BatchNorm1d(width))\n",
    "\n",
    "        self.convs  = nn.ModuleList(convs)\n",
    "        self.bns    = nn.ModuleList(bns)\n",
    "\n",
    "        self.conv3  = nn.Conv1d(width*scale, planes, kernel_size=1)\n",
    "        self.bn3    = nn.BatchNorm1d(planes)\n",
    "\n",
    "        self.relu   = nn.ReLU()\n",
    "\n",
    "        self.width  = width\n",
    "        self.se     = SEModule(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i==0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(sp)\n",
    "            sp = self.bns[i](sp)\n",
    "            if i==0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "                \n",
    "        out = torch.cat((out, spx[self.nums]),1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        out += residual\n",
    "\n",
    "        return out \n",
    "\n",
    "class ECAPA(nn.Module):\n",
    "    def __init__(self, \n",
    "                 C, \n",
    "                 model_scale, \n",
    "                 dim_out,\n",
    "                 features_dim, \n",
    "                 encoder_type=\"ASP\",\n",
    "                 context=False, \n",
    "                 summed=False, \n",
    "                 out_bn=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.context = context\n",
    "        self.summed = summed\n",
    "        self.features_dim = features_dim\n",
    "        self.encoder_type = encoder_type\n",
    "        self.out_bn = out_bn\n",
    "\n",
    "        self.scale  = model_scale\n",
    "\n",
    "        self.conv1  = nn.Conv1d(self.features_dim, C, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu   = nn.ReLU()\n",
    "        self.bn1    = nn.BatchNorm1d(C)\n",
    "        \n",
    "        self.layer1 = Bottle2neck(C, C, kernel_size=3, dilation=2, scale=self.scale)\n",
    "        self.layer2 = Bottle2neck(C, C, kernel_size=3, dilation=3, scale=self.scale)\n",
    "        self.layer3 = Bottle2neck(C, C, kernel_size=3, dilation=4, scale=self.scale)\n",
    "        self.layer4 = nn.Conv1d(3*C, 1536, kernel_size=1)\n",
    "\n",
    "        if self.context:\n",
    "            attn_input = 1536*3\n",
    "        else:\n",
    "            attn_input = 1536\n",
    "\n",
    "        if self.encoder_type == 'ECA':\n",
    "            attn_output = 1536\n",
    "        elif self.encoder_type == 'ASP':\n",
    "            attn_output = 1\n",
    "        else:\n",
    "            raise ValueError('Undefined encoder')\n",
    "\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(attn_input, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, attn_output, kernel_size=1),\n",
    "            nn.Softmax(dim=2),\n",
    "            )\n",
    "\n",
    "        self.bn5 = nn.BatchNorm1d(3072)\n",
    "\n",
    "        self.fc6 = nn.Linear(3072, dim_out)\n",
    "        if self.out_bn:\n",
    "            self.bn6 = nn.BatchNorm1d(dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        if self.summed:\n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x+x1)\n",
    "            x3 = self.layer3(x+x1+x2)\n",
    "        else:\n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x1)\n",
    "            x3 = self.layer3(x2)\n",
    "\n",
    "        x = self.layer4(torch.cat((x1,x2,x3),dim=1))\n",
    "        x = self.relu(x)\n",
    "\n",
    "        t = x.size()[-1]\n",
    "\n",
    "        if self.context:\n",
    "            global_x = torch.cat((x,torch.mean(x,dim=2,keepdim=True).repeat(1,1,t), torch.sqrt(torch.var(x,dim=2,keepdim=True).clamp(min=1e-4)).repeat(1,1,t)), dim=1)\n",
    "        else:\n",
    "            global_x = x\n",
    "\n",
    "        w = self.attention(global_x)\n",
    "        mu = torch.sum(x * w, dim=2)\n",
    "        sg = torch.sqrt( ( torch.sum((x**2) * w, dim=2) - mu**2 ).clamp(min=1e-4) )\n",
    "        x = torch.cat((mu, sg), 1)\n",
    "\n",
    "        x = self.bn5(x)\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        if self.out_bn:\n",
    "            x = self.bn6(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def ECAPA_1KCSBN(model_scale=8, C=128, **kwargs):\n",
    "    model = ECAPA(C=C, \n",
    "                  model_scale = model_scale, \n",
    "                  context=True, \n",
    "                  summed=True, \n",
    "                  out_bn=True, \n",
    "                  **kwargs)\n",
    "    return model\n",
    "\n",
    "class BCE(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, dim), requires_grad=True)\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.xavier_normal_(self.weight, gain=1)\n",
    "        \n",
    "    def logit(self, x):\n",
    "        return F.linear(x, self.weight)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x.astype(\"float32\")).cuda()\n",
    "        with torch.no_grad():\n",
    "            logit = self.logit(x)\n",
    "            \n",
    "        return self.sigmoid(logit)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        logit = self.logit(x)\n",
    "        return self.bce(logit, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:50.719336Z",
     "iopub.status.busy": "2022-07-06T23:16:50.718144Z",
     "iopub.status.idle": "2022-07-06T23:16:50.730337Z",
     "shell.execute_reply": "2022-07-06T23:16:50.729149Z",
     "shell.execute_reply.started": "2022-07-06T23:16:50.719284Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioReader:\n",
    "    def __init__(self, \n",
    "                 samplerate:int, \n",
    "                 length:int = None):\n",
    "        self.samplerate = samplerate\n",
    "        self.length = length\n",
    "        print(\"initialize AudioReader with samplerate: {}\".format(self.samplerate))\n",
    "        \n",
    "    def load_audio(self, file:str, frame_offset:int=None, num_frames:int=None):\n",
    "        audio, sr = torchaudio.load(file,\n",
    "                                    channels_first=True,\n",
    "                                    frame_offset=frame_offset,\n",
    "                                    num_frames=num_frames)\n",
    "        if sr != self.samplerate:\n",
    "            audio = F.resample(audio, sr,  self.samplerate)\n",
    "        if audio.shape[0] > 0:\n",
    "            audio = torch.mean(audio, dim=0, keepdims=True)\n",
    "        return audio\n",
    "\n",
    "    def __call__(self, file, length=None):\n",
    "        if length is None:\n",
    "            length = self.length\n",
    "        elif length == -1:\n",
    "            length = None\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if length is None:\n",
    "            audio = self.load_audio(file)\n",
    "            return audio\n",
    "        \n",
    "        info = torchaudio.info(file)\n",
    "        audio_length = info.num_frames\n",
    "\n",
    "        frame_offset_max = max(audio_length - length, 0)\n",
    "        if frame_offset_max > 0:\n",
    "            frame_offset = random.randint(0, frame_offset_max)\n",
    "        else:\n",
    "            frame_offset = 0\n",
    "\n",
    "        num_frames = min(audio_length, length)\n",
    "        audio = self.load_audio(file,\n",
    "                                frame_offset=frame_offset,\n",
    "                                num_frames=num_frames)\n",
    "\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Padder\n",
    "It's useful for training to augment audio wave witch too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:54.043708Z",
     "iopub.status.busy": "2022-07-06T23:16:54.043354Z",
     "iopub.status.idle": "2022-07-06T23:16:54.058712Z",
     "shell.execute_reply": "2022-07-06T23:16:54.057396Z",
     "shell.execute_reply.started": "2022-07-06T23:16:54.043677Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioPadder:\n",
    "    def __init__(self, mode, value=0.0, random=False, length=None):\n",
    "        \"\"\"\n",
    "        :param mode: mode of padding, pass to torch.pad.\n",
    "        :param value: value, works only for constant_padding mode\n",
    "        :param random: if true - left and right offset will be random, otherwise only right side pad\n",
    "        :param length: length of final audio in samples\n",
    "        \"\"\"\n",
    "        assert mode in ['constant', 'circular']\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.random = random\n",
    "        self.length = length\n",
    "\n",
    "    def circular_pad(self, x: torch.Tensor, pad: tuple):\n",
    "        is_squeeze = False\n",
    "        if len(x.shape) == 2:\n",
    "            x = x[None, ...]\n",
    "            is_squeeze = True\n",
    "        elif len(x.shape) == 3:\n",
    "            pass\n",
    "        else:\n",
    "            raise \"only 2D and 3D tensors supported\"\n",
    "\n",
    "        x_len = x.shape[-1]\n",
    "        pad_len_left = pad[0]\n",
    "        pad_len_right = pad[1]\n",
    "\n",
    "        n_repeat_left = pad_len_left // x_len\n",
    "        n_offset_left = pad_len_left % x_len\n",
    "        n_repeat_right = pad_len_right // x_len\n",
    "        n_offset_right = pad_len_right % x_len\n",
    "        n_repeat = n_repeat_left + n_repeat_right + 1\n",
    "        if n_repeat > 1:\n",
    "            x = torch.cat([x] * n_repeat, dim=-1)\n",
    "        x = F.pad(x, (n_offset_left, n_offset_right), mode=\"circular\")\n",
    "        if is_squeeze:\n",
    "            x = x.squeeze(0)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x, length=None):\n",
    "        if length is None:\n",
    "            length = self.length\n",
    "\n",
    "        if len(x.shape) < 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        length_x = x.shape[1]\n",
    "        if length_x >= length:\n",
    "            return x\n",
    "\n",
    "        if self.random:\n",
    "            pad_left = max(0, length - length_x)\n",
    "            pad_left = torch.randint(0, pad_left, (1,))\n",
    "        else:\n",
    "            pad_left = 0\n",
    "        pad_right = max(0, length - length_x - pad_left)\n",
    "        if self.mode == \"circular\":\n",
    "            x = self.circular_pad(x, pad=(pad_left, pad_right))\n",
    "        else:\n",
    "            x = F.pad(x, (pad_left, pad_right, 0, 0), mode=self.mode)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Extractor\n",
    "Extract audio from file\n",
    "use combination reader and padder to create extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:56.079289Z",
     "iopub.status.busy": "2022-07-06T23:16:56.076210Z",
     "iopub.status.idle": "2022-07-06T23:16:56.095250Z",
     "shell.execute_reply": "2022-07-06T23:16:56.093829Z",
     "shell.execute_reply.started": "2022-07-06T23:16:56.079243Z"
    }
   },
   "outputs": [],
   "source": [
    "class RawExtractor:\n",
    "    def __init__(self,\n",
    "                 length,\n",
    "                 dithering: float=None,\n",
    "                 samplerate: int=16000,\n",
    "                 normed: bool =False\n",
    "                 ):\n",
    "        self.dithering = dithering\n",
    "        self.length = length\n",
    "        self.samplerate = samplerate\n",
    "        self.normed = normed       \n",
    "        self.audio_loader = AudioReader(samplerate=self.samplerate,\n",
    "                                        length=length)\n",
    "        \n",
    "        if length is not None:\n",
    "            self.audio_padder = AudioPadder(mode=\"constant\",\n",
    "                                            length=length,\n",
    "                                            random=True\n",
    "                                            )\n",
    "        else:\n",
    "            self.audio_padder = None\n",
    "\n",
    "    def make_dithering(self, audio):\n",
    "        return audio + self.dithering * torch.rand_like(audio)\n",
    "    \n",
    "    def normalize(self, audio):\n",
    "        audio = audio - torch.mean(audio)\n",
    "        audio = audio / (torch.std(audio) + 1e-6)\n",
    "        return audio\n",
    "        \n",
    "    def __call__(self, sound):\n",
    "        if not isinstance(sound, str):\n",
    "            sound = sound.path\n",
    "            \n",
    "        audio = self.audio_loader(file=sound, length=self.length)\n",
    "            \n",
    "        if self.normed:\n",
    "            audio = self.normalize(audio)\n",
    "            \n",
    "        if self.audio_padder is not None:\n",
    "            audio = self.audio_padder(x=audio, length=self.length)\n",
    "            \n",
    "        if self.dithering is not None:\n",
    "            audio = self.make_dithering(audio)\n",
    "            \n",
    "        return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:58.034718Z",
     "iopub.status.busy": "2022-07-06T23:16:58.034061Z",
     "iopub.status.idle": "2022-07-06T23:16:58.047111Z",
     "shell.execute_reply": "2022-07-06T23:16:58.046085Z",
     "shell.execute_reply.started": "2022-07-06T23:16:58.034683Z"
    }
   },
   "outputs": [],
   "source": [
    "class MFCC(nn.Module):\n",
    "    def __init__(self,\n",
    "                 win_length: int,\n",
    "                 hop_length: int,\n",
    "                 n_fft: int, \n",
    "                 n_mels: int, \n",
    "                 n_mfcc: int,\n",
    "                 f_min: float, \n",
    "                 f_max: float, \n",
    "                 sample_rate: int,\n",
    "                 log_mels=False,\n",
    "                 instancenorm=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max\n",
    "        self.n_mels = n_mels\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_fn = torch.hamming_window\n",
    "        self.log_mels = log_mels\n",
    "        \n",
    "        mfcc = torchaudio.transforms.MFCC\n",
    "        self.extractor = mfcc(sample_rate=self.sample_rate, \n",
    "                              n_mfcc = self.n_mfcc, \n",
    "                              log_mels=self.log_mels, \n",
    "                              dct_type = 2, \n",
    "                              melkwargs={'n_mels': self.n_mels, \n",
    "                                         'n_fft':self.n_fft, \n",
    "                                         'win_length':self.win_length, \n",
    "                                         'hop_length':self.hop_length, \n",
    "                                         'f_min':self.f_min, \n",
    "                                         'f_max':self.f_max, \n",
    "                                         'window_fn':self.window_fn})\n",
    "        \n",
    "        if instancenorm:\n",
    "            self.instancenorm   = nn.InstanceNorm1d(self.n_mels)\n",
    "        else:\n",
    "            self.instancenorm = None\n",
    "        \n",
    "    def dim(self):\n",
    "        return self.n_mfcc\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            if x.dim() == 3:\n",
    "                x = x.squeeze(1)\n",
    "\n",
    "            f = self.extractor(x)\n",
    "            if self.instancenorm is not None:\n",
    "                f = self.instancenorm(f)\n",
    "\n",
    "            return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:16:59.973205Z",
     "iopub.status.busy": "2022-07-06T23:16:59.972189Z",
     "iopub.status.idle": "2022-07-06T23:16:59.985509Z",
     "shell.execute_reply": "2022-07-06T23:16:59.984346Z",
     "shell.execute_reply.started": "2022-07-06T23:16:59.973150Z"
    }
   },
   "outputs": [],
   "source": [
    "class Fbank(nn.Module):\n",
    "    def __init__(self, \n",
    "                 win_length: int,\n",
    "                 hop_length: int,\n",
    "                 n_fft: int, \n",
    "                 f_min: float, \n",
    "                 f_max: float, \n",
    "                 n_mels: int, \n",
    "                 sample_rate: int,\n",
    "                 log_input=False,\n",
    "                 instancenorm=False,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max\n",
    "        self.n_mels = n_mels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.log_input = log_input\n",
    "        self.window_fn = torch.hamming_window\n",
    "        \n",
    "        self.extractor = T.MelSpectrogram(n_fft=self.n_fft,\n",
    "                                          win_length = self.win_length,\n",
    "                                          hop_length = self.hop_length,\n",
    "                                          f_min = self.f_min,\n",
    "                                          f_max = self.f_max,\n",
    "                                          n_mels = self.n_mels,\n",
    "                                          sample_rate = self.sample_rate,\n",
    "                                          window_fn = self.window_fn,\n",
    "                                          **kwargs)\n",
    "        if instancenorm:\n",
    "            self.instancenorm   = nn.InstanceNorm1d(self.n_mels)\n",
    "        else:\n",
    "            self.instancenorm = None\n",
    "\n",
    "    def dim(self):\n",
    "        return self.n_mels\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            if x.dim() == 3:\n",
    "                x = x.squeeze(1)\n",
    "\n",
    "            f = self.extractor(x)\n",
    "            if self.log_input:\n",
    "                f = (f + 1e-7).log()\n",
    "\n",
    "            if self.instancenorm is not None:\n",
    "                f = self.instancenorm(f)\n",
    "\n",
    "            return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Domain Augmentations\n",
    "simgple noise mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:02.104452Z",
     "iopub.status.busy": "2022-07-06T23:17:02.103940Z",
     "iopub.status.idle": "2022-07-06T23:17:02.117069Z",
     "shell.execute_reply": "2022-07-06T23:17:02.116128Z",
     "shell.execute_reply.started": "2022-07-06T23:17:02.104410Z"
    }
   },
   "outputs": [],
   "source": [
    "class NoiseAugment(nn.Module):\n",
    "    def __init__(self, snr_db_min: float, snr_db_max: float, dim=-1):\n",
    "        super().__init__()\n",
    "        assert snr_db_min < snr_db_max\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.snr_db_min = snr_db_min\n",
    "        self.snr_db_max = snr_db_max\n",
    "        self.snr_gap = snr_db_max - snr_db_min\n",
    "        \n",
    "    @staticmethod\n",
    "    def db_2_mag(db):\n",
    "        return 20 ** (db / 20)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            snr_db = self.snr_db_min + torch.rand(x.shape[0]) * self.snr_gap\n",
    "            snr_linear = self.db_2_mag(snr_db).to(x.dtype).to(x.device)\n",
    "\n",
    "            std_x = torch.std(x, dim=self.dim, keepdims=True)\n",
    "            scale = std_x.squeeze() / snr_linear\n",
    "            scale = scale.unsqueeze(1).unsqueeze(1)\n",
    "            return x + scale * torch.rand_like(x).to(x.dtype).to(x.device)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Domain Augmentations\n",
    "> [SpecAugment](https://arxiv.org/abs/1904.08779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:03.901584Z",
     "iopub.status.busy": "2022-07-06T23:17:03.901125Z",
     "iopub.status.idle": "2022-07-06T23:17:03.918461Z",
     "shell.execute_reply": "2022-07-06T23:17:03.917035Z",
     "shell.execute_reply.started": "2022-07-06T23:17:03.901543Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self, \n",
    "                 time_mask_param: int, \n",
    "                 freq_mask_param: int,\n",
    "                 p_time_mask=1.0,\n",
    "                 p_freq_mask=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert 0 <= p_time_mask <= 1.0\n",
    "        assert 0 <= p_freq_mask <= 1.0\n",
    "        \n",
    "        self.__time_mask_param = time_mask_param\n",
    "        self.__freq_mask_param = freq_mask_param\n",
    "        \n",
    "        self.p_time_mask = p_time_mask\n",
    "        self.p_freq_mask = p_freq_mask\n",
    "        \n",
    "        self.time_mask_fun = T.TimeMasking(time_mask_param=time_mask_param)\n",
    "        self.freq_mask_fun = T.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        _repr = \"SpecAugment(time_mask_param={}, freq_mask_param={})\".format(self.__time_mask_param, \n",
    "                                                                             self.__freq_mask_param)\n",
    "        return _repr\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            for n in range(x.shape[0]):\n",
    "                if torch.rand(1) <= self.p_time_mask:\n",
    "                    x[n] = self.time_mask_fun(x[n].unsqueeze(0))\n",
    "                if torch.rand(1) <= self.p_freq_mask:\n",
    "                    x[n] = self.freq_mask_fun(x[n].unsqueeze(0))\n",
    "            return x\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create class which provide data loading ruls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:05.950592Z",
     "iopub.status.busy": "2022-07-06T23:17:05.950245Z",
     "iopub.status.idle": "2022-07-06T23:17:05.968494Z",
     "shell.execute_reply": "2022-07-06T23:17:05.967507Z",
     "shell.execute_reply.started": "2022-07-06T23:17:05.950562Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetAudio(Dataset):\n",
    "    def __init__(self, \n",
    "                 sounds_list,\n",
    "                 extractor,\n",
    "                 batch_size:int,\n",
    "                 mode=\"train\",\n",
    "                 length:tuple=None  # you can use it as dinamic length of extractor\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert mode in [\"train\", \"test\", \"valid\"]\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.sounds_list = sounds_list\n",
    "        self.extractor = extractor\n",
    "        if mode == \"train\":\n",
    "            self.batch_size = batch_size\n",
    "        else:\n",
    "            self.batch_size = 1\n",
    "        self.length = length\n",
    "        self.id_map = {\"human\": 0.0, \"spoof\": 1.0}\n",
    "        \n",
    "        print(\"Init DatasetAudio with batch size: {:d}\".format(self.batch_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sounds_list) // self.batch_size\n",
    "    \n",
    "    def get_item_train(self, index):\n",
    "        if self.batch_size is None:\n",
    "            sound_item = self.sounds_list[index]\n",
    "            x = self.extractor(sound=sound_item)\n",
    "            y = [self.id_map[sound_item.sound_id]]\n",
    "            y = torch.FloatTensor(y)\n",
    "        else:\n",
    "            sounds_batch = random.choices(self.sounds_list, k=self.batch_size)\n",
    "            x = list()\n",
    "            y = list()\n",
    "            for sound_item in sounds_batch:\n",
    "                x.append(self.extractor(sound=sound_item))\n",
    "                y.append(self.id_map[sound_item.sound_id])\n",
    "            x = torch.stack(x)\n",
    "            y = torch.FloatTensor(y).unsqueeze(1)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def get_item_valid(self, index):\n",
    "        sound_item = self.sounds_list[index]\n",
    "        x = self.extractor(sound=sound_item)\n",
    "        y = [self.id_map[sound_item.sound_id]]\n",
    "        y = torch.FloatTensor(y)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def get_item_test(self, index):\n",
    "        sound_item = self.sounds_list[index]\n",
    "        x = self.extractor(sound=sound_item)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == \"train\":\n",
    "            return self.get_item_train(index)\n",
    "        elif self.mode == \"valid\":\n",
    "            return self.get_item_valid(index=index)\n",
    "        elif self.mode == \"test\":\n",
    "            return self.get_item_test(index=index)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:07.790613Z",
     "iopub.status.busy": "2022-07-06T23:17:07.790222Z",
     "iopub.status.idle": "2022-07-06T23:17:07.810302Z",
     "shell.execute_reply": "2022-07-06T23:17:07.809300Z",
     "shell.execute_reply.started": "2022-07-06T23:17:07.790571Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 loss,\n",
    "                 optimizer_fn,\n",
    "                 autocast:bool = False,\n",
    "                 validation=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer_fn(self.parameters())\n",
    "        self.validation = validation\n",
    "        \n",
    "        self.autocast = autocast\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        self.meter_loss = AverageMeter(name=\"loss\", momentum=0.9)\n",
    "        self.meter_accuracy = AverageMeter(name=\"accuracy\", momentum=0.9)\n",
    "        self.meter_speed = AverageMeter(name=\"speed\", momentum=0.5)\n",
    "        self.timer = Timer()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.meter_loss.reset()\n",
    "        self.meter_accuracy.reset()\n",
    "        self.meter_speed.reset()\n",
    "        self.train()\n",
    "        self.timer.tic()\n",
    "        \n",
    "    def show(self, epoch, n_step, steps_total, loss, accuracy):\n",
    "        duration = self.timer.tictoc()\n",
    "        speed = 1.0 / duration\n",
    "        eta = (steps_total - n_step) / speed\n",
    "        \n",
    "        self.meter_loss.step(loss)\n",
    "        self.meter_accuracy.step(accuracy)\n",
    "        self.meter_speed.step(speed)\n",
    "        \n",
    "        info = \" \".join(\n",
    "            [\n",
    "                \"\\repoch-{:d} ->\".format(epoch),\n",
    "                \"{:d}\\{:d}\".format(n_step, steps_total),\n",
    "                \"loss: {:0.3f}\".format(self.meter_loss.get()),\n",
    "                \"accuracy: {:0.3f}\".format(self.meter_accuracy.get()),\n",
    "                \"speed: {:0.1f} it/sec\".format(self.meter_speed.get()),\n",
    "                \"ETA: {:0.1f}\".format(eta)\n",
    "            ])\n",
    "        print(info, end='\\r', flush=True)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(x)\n",
    "            pred = self.loss.predict(out)\n",
    "        self.train()\n",
    "        return pred\n",
    "        \n",
    "    def step_epoch(self, num_epoch, dataloader):\n",
    "        self.reset()\n",
    "        \n",
    "        iterator = iter(dataloader)\n",
    "        steps_epoch = len(iterator)\n",
    "        \n",
    "        for n_step in range(steps_epoch):\n",
    "            self.zero_grad()\n",
    "            loss, acc = self.step(iterator)\n",
    "            \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            loss_detached = float(loss.detach().cpu())\n",
    "            accuracy = float(acc.cpu())\n",
    "            self.show(epoch=num_epoch,\n",
    "                      n_step=n_step + 1,\n",
    "                      steps_total=steps_epoch,\n",
    "                      loss=loss_detached,\n",
    "                      accuracy=accuracy\n",
    "                      )\n",
    "        if self.validation is not None:\n",
    "            if (num_epoch % self.validation.period) == 0:\n",
    "                *_, auc_score, logloss = self.validation(self)\n",
    "                print(\"validation auc score: {:0.3f}\".format(auc_score))\n",
    "                print(\"validation logloss: {:0.3f}\".format(logloss))\n",
    "            \n",
    "    def step(self, iterator):\n",
    "        x, y = next(iterator)\n",
    "        x = x.cuda()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=self.autocast):\n",
    "            out = self.model.forward(x)\n",
    "            l = self.loss.forward(out, y.to(out.device))\n",
    "            y_pred = self.loss.predict(out.detach())\n",
    "        acc = accuracy(y_pred, y.to(y_pred.device))\n",
    "        \n",
    "        return l, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester\n",
    "Make test and validation in single class :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:09.849339Z",
     "iopub.status.busy": "2022-07-06T23:17:09.848878Z",
     "iopub.status.idle": "2022-07-06T23:17:09.867450Z",
     "shell.execute_reply": "2022-07-06T23:17:09.866450Z",
     "shell.execute_reply.started": "2022-07-06T23:17:09.849305Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self, dataset, save_folder, mode, period=1):\n",
    "        assert mode in [\"test\", \"valid\"]\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.dataset = dataset\n",
    "        self.save_folder = save_folder\n",
    "        self.best_auc = 0.0\n",
    "        self.best_logloss = np.inf\n",
    "        self.period = period\n",
    "        \n",
    "        if self.save_folder is not None:\n",
    "            os.makedirs(self.save_folder, exist_ok=True)\n",
    "        \n",
    "    def get_loader(self):\n",
    "        loader = DataLoader(dataset=self.dataset,\n",
    "                            shuffle=False,\n",
    "                            batch_size=None,\n",
    "                            num_workers=8,\n",
    "                            prefetch_factor=1,\n",
    "                            persistent_workers=False,\n",
    "                            pin_memory=True)\n",
    "        return loader\n",
    "        \n",
    "    def call_valid(self, engine):\n",
    "        loader = self.get_loader()\n",
    "        iterator = iter(loader)\n",
    "        \n",
    "        y_true = list()\n",
    "        y_pred = list()\n",
    "        for x, y in tqdm(iterator, desc=\"evaluating\"):\n",
    "            y_true.append(y.squeeze().cpu())\n",
    "            y_pred.append(engine.predict(x.cuda()).squeeze().cpu().numpy())\n",
    "        y_true = np.hstack(y_true)\n",
    "        y_pred = np.hstack(y_pred)\n",
    "\n",
    "        auc_score = roc_auc_score(y_true, y_pred)\n",
    "        logloss = log_loss(y_true, y_pred, eps=1e-4)\n",
    "        \n",
    "        if self.save_folder is not None:\n",
    "            if auc_score > self.best_auc:\n",
    "                self.best_auc = auc_score\n",
    "                file_save = os.path.join(self.save_folder, \"best-auc.torch\")\n",
    "                torch.save(engine.state_dict(), file_save)\n",
    "                print(\"save model to {} with best auc score {:0.3f}\".format(file_save, auc_score))\n",
    "                \n",
    "            if logloss < self.best_logloss:\n",
    "                self.best_logloss = logloss\n",
    "                file_save = os.path.join(self.save_folder, \"best-logloss.torch\")\n",
    "                torch.save(engine.state_dict(), file_save)\n",
    "                print(\"save model to {} with best logloss {:0.3f}\".format(file_save, logloss))\n",
    "        \n",
    "        del loader\n",
    "        gc.collect()\n",
    "        \n",
    "        return (y_true, y_pred), auc_score, logloss\n",
    "        \n",
    "    def call_test(self, engine):\n",
    "        loader = self.get_loader()\n",
    "        iterator = iter(loader)\n",
    "        \n",
    "        y_pred = list()\n",
    "        for x in tqdm(iterator):\n",
    "            proba = engine.predict(x.cuda()).cpu()\n",
    "            y_pred.append(float(proba))\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, engine):\n",
    "        if self.mode == \"valid\":\n",
    "            return self.call_valid(engine=engine)\n",
    "        elif self.mode == \"test\":\n",
    "            return self.call_test(engine=engine)\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:13.087127Z",
     "iopub.status.busy": "2022-07-06T23:17:13.086242Z",
     "iopub.status.idle": "2022-07-06T23:17:13.092710Z",
     "shell.execute_reply": "2022-07-06T23:17:13.091491Z",
     "shell.execute_reply.started": "2022-07-06T23:17:13.087090Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLERATE = 16000\n",
    "DITHERING = 1e-7\n",
    "dim_embeddings = 256\n",
    "\n",
    "par_dir = \"/kaggle/input/made-voice-anti-spoofing-parctice/\"\n",
    "save_folder = \"results\"\n",
    "data_path_train = os.path.join(par_dir, \"train\")\n",
    "data_path_test = os.path.join(par_dir, \"test\")\n",
    "meta_path = os.path.join(par_dir, \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:16.871957Z",
     "iopub.status.busy": "2022-07-06T23:17:16.871605Z",
     "iopub.status.idle": "2022-07-06T23:17:36.088906Z",
     "shell.execute_reply": "2022-07-06T23:17:36.087881Z",
     "shell.execute_reply.started": "2022-07-06T23:17:16.871928Z"
    }
   },
   "outputs": [],
   "source": [
    "files_train = sorted(find_files(data_path_train, pattern=\"**/*.wav\"))\n",
    "files_test = sorted(find_files(data_path_test, pattern=\"**/*.wav\"))\n",
    "\n",
    "meta = pd.read_csv(meta_path, index_col=\"filename\").to_dict()[\"class_id\"]\n",
    "sounds = list()\n",
    "for f in files_train:\n",
    "    sounds.append(Sound(path=f, sound_id=meta[stem(f)]))\n",
    "    \n",
    "sounds_test = list()\n",
    "for f in files_test:\n",
    "    sounds_test.append(Sound(path=f, sound_id=None))\n",
    "    \n",
    "sounds_train, sounds_valid = train_test_split(sounds, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Extractors\n",
    "Be carefull, extractors for train and test is different, but some parameters like \"dithering\" or \"sameple rate\" must be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:53.601099Z",
     "iopub.status.busy": "2022-07-06T23:17:53.600682Z",
     "iopub.status.idle": "2022-07-06T23:17:53.627321Z",
     "shell.execute_reply": "2022-07-06T23:17:53.626272Z",
     "shell.execute_reply.started": "2022-07-06T23:17:53.601068Z"
    }
   },
   "outputs": [],
   "source": [
    "extractor_audio_train = RawExtractor(\n",
    "    length=int(SAMPLERATE * 3.0),\n",
    "    dithering=1e-6,\n",
    "    samplerate=16000,\n",
    "    normed=True)\n",
    "extractor_audio_test = RawExtractor(\n",
    "    length=None,\n",
    "    dithering=DITHERING,\n",
    "    samplerate=16000,\n",
    "    normed=True)\n",
    "extractor_spec =Fbank(\n",
    "    win_length=int(0.025 * SAMPLERATE),\n",
    "    hop_length=int(0.01 * SAMPLERATE),\n",
    "    n_fft=512, \n",
    "    n_mels=64, \n",
    "    f_min=200, \n",
    "    f_max=7000, \n",
    "    sample_rate=SAMPLERATE,\n",
    "    log_input=True,\n",
    "    instancenorm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:57.471600Z",
     "iopub.status.busy": "2022-07-06T23:17:57.470886Z",
     "iopub.status.idle": "2022-07-06T23:17:57.516966Z",
     "shell.execute_reply": "2022-07-06T23:17:57.515945Z",
     "shell.execute_reply.started": "2022-07-06T23:17:57.471565Z"
    }
   },
   "outputs": [],
   "source": [
    "model_base = ECAPA_1KCSBN(\n",
    "    features_dim=extractor_spec.dim(),\n",
    "    dim_out=dim_embeddings)\n",
    "features_extractor = nn.Sequential(\n",
    "    NoiseAugment(snr_db_min=12, snr_db_max=20),\n",
    "    extractor_spec,\n",
    "    SpecAugment(time_mask_param=40, freq_mask_param=8))\n",
    "model = nn.Sequential(\n",
    "    features_extractor,\n",
    "    model_base\n",
    ")\n",
    "loss = BCE(dim=dim_embeddings)\n",
    "optimizer_fn = lambda x: torch.optim.Adam(x, lr=0.0003)\n",
    "# scheduller = \n",
    "sheduler = torch.optim.lr_scheduler.StepLR(optimizer_fn(model.parameters()), step_size=1, gamma=0.5, last_epoch=- 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:17:59.540372Z",
     "iopub.status.busy": "2022-07-06T23:17:59.539679Z",
     "iopub.status.idle": "2022-07-06T23:17:59.548231Z",
     "shell.execute_reply": "2022-07-06T23:17:59.547160Z",
     "shell.execute_reply.started": "2022-07-06T23:17:59.540333Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train = DatasetAudio( \n",
    "    sounds_train,\n",
    "    extractor=extractor_audio_train,\n",
    "    batch_size=64,\n",
    "    mode=\"train\",\n",
    "    length=None)\n",
    "dataset_valid = DatasetAudio( \n",
    "    sounds_valid,\n",
    "    extractor=extractor_audio_test,\n",
    "    batch_size=None,\n",
    "    mode=\"valid\",\n",
    "    length=None)\n",
    "dataset_test = DatasetAudio( \n",
    "    sounds_test,\n",
    "    extractor=extractor_audio_test,\n",
    "    batch_size=None,\n",
    "    mode=\"test\",\n",
    "    length=None)\n",
    "\n",
    "loader_train = DataLoader(dataset=dataset_train,\n",
    "                          batch_size=None,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2,\n",
    "                          prefetch_factor=1,\n",
    "                          persistent_workers=True,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:18:01.473804Z",
     "iopub.status.busy": "2022-07-06T23:18:01.473448Z",
     "iopub.status.idle": "2022-07-06T23:18:07.506314Z",
     "shell.execute_reply": "2022-07-06T23:18:07.500106Z",
     "shell.execute_reply.started": "2022-07-06T23:18:01.473774Z"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(loader_train)\n",
    "batch_to_check = next(iterator)[0]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for n in range(8):\n",
    "    wave = batch_to_check[n]\n",
    "    features_clean = features_extractor.eval()(wave)[0].cpu().numpy()\n",
    "    features_noisy = features_extractor.train()(wave)[0].cpu().numpy()\n",
    "\n",
    "    plt.subplot(8, 2, 2 * n + 1)\n",
    "    plt.pcolormesh(features_clean)\n",
    "    plt.subplot(8, 2, 2 * n + 2)\n",
    "    plt.pcolormesh(features_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Testers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:18:07.508887Z",
     "iopub.status.busy": "2022-07-06T23:18:07.508500Z",
     "iopub.status.idle": "2022-07-06T23:18:07.552994Z",
     "shell.execute_reply": "2022-07-06T23:18:07.551024Z",
     "shell.execute_reply.started": "2022-07-06T23:18:07.508851Z"
    }
   },
   "outputs": [],
   "source": [
    "validation = Tester(\n",
    "    dataset=dataset_valid,\n",
    "    period=1,\n",
    "    save_folder=save_folder,\n",
    "    mode=\"valid\"\n",
    "    )\n",
    "evaluation = Tester(\n",
    "    dataset=dataset_test,\n",
    "    period=None,\n",
    "    save_folder=None,\n",
    "    mode=\"test\"\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    autocast=False,\n",
    "    validation=validation,\n",
    "    optimizer_fn=optimizer_fn).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T23:18:10.459745Z",
     "iopub.status.busy": "2022-07-06T23:18:10.459370Z",
     "iopub.status.idle": "2022-07-07T00:21:52.983633Z",
     "shell.execute_reply": "2022-07-07T00:21:52.981347Z",
     "shell.execute_reply.started": "2022-07-06T23:18:10.459712Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "for n in range(n_epochs):\n",
    "    trainer.step_epoch(n, dataloader=loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T00:25:58.050139Z",
     "iopub.status.busy": "2022-07-07T00:25:58.049203Z",
     "iopub.status.idle": "2022-07-07T00:28:13.880348Z",
     "shell.execute_reply": "2022-07-07T00:28:13.879133Z",
     "shell.execute_reply.started": "2022-07-07T00:25:58.050102Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.load_state_dict(torch.load(os.path.join(save_folder, \"best-auc.torch\")))\n",
    "scores_test = evaluation(trainer)\n",
    "filenames = [stem(x.path) for x in sounds_test]\n",
    "save_file_csv = os.path.join(save_folder, \"submission.csv\")\n",
    "pd.DataFrame({\"filename\": filenames, \"score\": scores_test}).to_csv(save_file_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Next?\n",
    "1. you can try to use different features (mfcc, fbanks, spectrogramm) with different parameters\n",
    "2. Use neural network which works with raw singal ([rawnet](https://github.com/Jungjee/RawNet), [sincnet](https://arxiv.org/abs/1808.00158), etc.)\n",
    "3. Use Image Processing models (efficientnet, resnet, squeezenet, mobilenet, etc.) be carefull by default that model used features dimension == 3, but spectral featrues dim == 1\n",
    "4. Use Real Noise instead of syntetic, just reimplement \"NoiseAugment\" to use different real life noises instead of torch.rand. You may use Musan dataset ([link kaggle](https://www.kaggle.com/datasets/nhattruongdev/musan-noise), [link openslr](https://www.openslr.org/17/)) for this\n",
    "5. Add pitch for specaug\n",
    "6. Use K-fold technique\n",
    "7. Voice Activiti Detection make a sense\n",
    "8. Fusion or stacking\n",
    "\n",
    "GL;HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
